{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_test = pd.read_csv(\"./datasets/temp_cleaned_reviews_dataset.csv\")\n",
    "# print(clean_test[\"PREPROCESSED_REVIEW\"][0])\n",
    "# sentences = []\n",
    "# for sent in clean_test[\"PREPROCESSED_REVIEW\"]:\n",
    "#     sentences.append(sent.strip('][').replace(\"'\",\"\").split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./datasets/all_cleaned_reviews_fantasy.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1119852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html#introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus_file (str, optional) – Path to a corpus file in LineSentence format. You may use this argument instead of sentences to get performance boost. Only one of sentences or corpus_file arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
    "\n",
    "LineSentece format is words seperated by whitespace, one sentence per line.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "\n",
    "window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used. hierarchical softmax is allegedly better for infrequent words, so maybe we use this? maybe not?\n",
    "\n",
    "sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "Everything else seems reasonable for just using standard values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 0 for CBOW, 1 for skipgram\n",
    "#skipgram is used in Lucy2020 model\n",
    "sg = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = brown.sents()\n",
    "sents = sentences\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=sents, vector_size=100, window=5, min_count=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "word_vectors.save(\"word2vec.wordvectors1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
