{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca2618/COMP550_project/blob/main/data/preprocess_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Il4s_vHhFaj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nKJMJPjMzJj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69284810-4e60-4a7a-a69c-fa6e89ce9fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "import math\n",
        "import regex as re\n",
        "import ast\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "1jFpPaq5PE03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning github repo\n",
        "!git clone https://github.com/luca2618/COMP550_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw7OFWqMcn0e",
        "outputId": "c9f9c588-a521-4c0a-a9ee-26134814e1ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMP550_project'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 79 (delta 8), reused 18 (delta 6), pack-reused 59\u001b[K\n",
            "Receiving objects: 100% (79/79), 376.51 MiB | 33.86 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping dataset and saving it in a dataframe\n",
        "!unzip COMP550_project/data/reviews_dataset.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3I5o9aVP3YQ",
        "outputId": "52479712-a439-4764-e34d-cf335dae28d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  COMP550_project/data/reviews_dataset.csv.zip\n",
            "  inflating: reviews_dataset.csv     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv('reviews_dataset.csv')"
      ],
      "metadata": {
        "id": "es9ixL9UgH_x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "NR5sPRJlrsED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### converts column of lists as string to list\n",
        "def convert_string_list_col(df):\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    df.at[index, 'PREPROCESSED_REVIEW'] = ast.literal_eval(row['PREPROCESSED_REVIEW'])\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "YAA6UmzdXuqb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### returns text as tokens\n",
        "def preprocess_text(text):\n",
        "\n",
        "  # removes punctuation and numbers\n",
        "  text = re.sub('[0-9_;:!?.,*@#\\[\\]\\(\\)?<>\"\\']', '', text)\n",
        "  lowercase_text = text.lower()\n",
        "\n",
        "  # tokenizes the text\n",
        "  tokens = word_tokenize(lowercase_text)\n",
        "\n",
        "  ### for now, dont remove stopwords as we need \"she\", \"her\", etc.\n",
        "  # removes stopwords\n",
        "  # tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "  ### for now, dont lemmatize words as our axes seem to focus on adjectives\n",
        "  # lemmatizes words\n",
        "  # tokens = lemmatize_text_tokens(tokens)\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "mTNpdlBjsHjU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text_tokens(text_tokens):\n",
        "  # intializes lemmatizer\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  # pos tags\n",
        "  pos_tags = nltk.pos_tag(text_tokens)\n",
        "  # replaces NLTK tags with ones that are accepted by WordNetLemmatizer\n",
        "  pos_tags = [(word, lemmatizer_pos(tag)) for word, tag in pos_tags]\n",
        "\n",
        "  # lemmatizes each word\n",
        "  lemmatized_tokens = [wnl.lemmatize(word, tag) for word, tag in pos_tags]\n",
        "\n",
        "  return lemmatized_tokens"
      ],
      "metadata": {
        "id": "iFP7330H6krS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer_pos(pos):\n",
        "\n",
        "  pos_first_letter = pos[0]\n",
        "\n",
        "  match pos_first_letter:\n",
        "    # verbs\n",
        "    case 'V':\n",
        "      pos = 'v'\n",
        "    # adjectives\n",
        "    case 'J':\n",
        "      pos = 'a'\n",
        "    # adverbs\n",
        "    case 'R':\n",
        "      pos = 'r'\n",
        "    case _:\n",
        "      pos = 'n'\n",
        "\n",
        "  return pos"
      ],
      "metadata": {
        "id": "vFQHx6nGAFau"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### adds a new column for the preprocessed review and populates it\n",
        "def preprocess_reviews_text(df, verbose = True):\n",
        "  # for tracking progress\n",
        "  count = 0\n",
        "\n",
        "  if 'PREPROCESSED_REVIEW' not in df.columns:\n",
        "    df['PREPROCESSED_REVIEW'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "    # for tracking progress\n",
        "    if count % 10000 == 0 and verbose:\n",
        "      print(count)\n",
        "\n",
        "    review_text = row['REVIEW']\n",
        "\n",
        "    preprocessed_review = preprocess_text(review_text)\n",
        "    df.at[index, 'PREPROCESSED_REVIEW'] = preprocessed_review\n",
        "\n",
        "    # for tracking progress\n",
        "    count += 1\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "eQgP6iF7qy1Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesses reviews text"
      ],
      "metadata": {
        "id": "Qadj5KlCJJT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### preprocesses reviews\n",
        "cleaned_df = preprocess_reviews_text(reviews_df)"
      ],
      "metadata": {
        "id": "hOtV1c21q46Q",
        "outputId": "834d7168-5ab2-4711-f220-be39936309ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "60000\n",
            "70000\n",
            "80000\n",
            "90000\n",
            "100000\n",
            "110000\n",
            "120000\n",
            "130000\n",
            "140000\n",
            "150000\n",
            "160000\n",
            "170000\n",
            "180000\n",
            "190000\n",
            "200000\n",
            "210000\n",
            "220000\n",
            "230000\n",
            "240000\n",
            "250000\n",
            "260000\n",
            "270000\n",
            "280000\n",
            "290000\n",
            "300000\n",
            "310000\n",
            "320000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### removing raw text from datatframe and saving it as a json file\n",
        "cleaned_df = cleaned_df.drop(columns = ['REVIEW'])\n",
        "json_cleaned_reviews_dataset = cleaned_df.to_json(orient = \"records\")"
      ],
      "metadata": {
        "id": "1GNZfEdwnLKs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving files"
      ],
      "metadata": {
        "id": "gwgS9wcffeNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### downloading cleaned dataset (google colab)\n",
        "\n",
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "with open('preprocessed_reviews_dataset.json', 'w') as f:\n",
        "  json.dump(json_cleaned_reviews_dataset, f)\n",
        "  files.download('preprocessed_reviews_dataset.json')"
      ],
      "metadata": {
        "id": "cSp2wGiefhB0",
        "outputId": "25ac888b-ecea-4716-b082-27ea1f4df38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68d53f07-d47f-4f51-ad5d-7870bf0c53cc\", \"preprocessed_reviews_dataset_1.json\", 301248397)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa285a41-5563-4521-9477-5309a7ecf397\", \"preprocessed_reviews_dataset_2.json\", 302054499)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}