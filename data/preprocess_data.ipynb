{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luca2618/COMP550_project/blob/main/preprocess_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Il4s_vHhFaj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nKJMJPjMzJj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528e056b-28a0-4916-b290-dea23b74fffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "### importing libraries\n",
        "\n",
        "# basic libraries\n",
        "import pandas as pd\n",
        "import math\n",
        "import regex as re\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# to download files\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing dataset"
      ],
      "metadata": {
        "id": "1jFpPaq5PE03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cloning github repo\n",
        "!git clone https://github.com/luca2618/COMP550_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw7OFWqMcn0e",
        "outputId": "7eee17a8-7ad8-49ed-f65c-0dc995930831"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'COMP550_project'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 50 (delta 10), reused 21 (delta 4), pack-reused 17\u001b[K\n",
            "Receiving objects: 100% (50/50), 135.56 MiB | 20.52 MiB/s, done.\n",
            "Resolving deltas: 100% (14/14), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping dataset and saving it in a dataframe\n",
        "!unzip COMP550_project/data/goodreads_reviews_dataset.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3I5o9aVP3YQ",
        "outputId": "26f90d0f-7dd1-406e-ad31-56260b957116"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  COMP550_project/data/goodreads_reviews_dataset.csv.zip\n",
            "  inflating: goodreads_reviews_dataset.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv('goodreads_reviews_dataset.csv')"
      ],
      "metadata": {
        "id": "es9ixL9UgH_x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing"
      ],
      "metadata": {
        "id": "NR5sPRJlrsED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### returns text as tokens\n",
        "def preprocess_text(text):\n",
        "\n",
        "  # removes punctuation and numbers\n",
        "  text = re.sub('[0-9_;:!?.,*@#\\[\\]\\(\\)?<>\"\\']', '', text)\n",
        "  lowercase_text = text.lower()\n",
        "\n",
        "  # tokenizes the text\n",
        "  tokens = word_tokenize(lowercase_text)\n",
        "\n",
        "  # removes stopwords\n",
        "  tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "  ### for now, dont lemmatize words as our axes seem to focus on adjectives\n",
        "  # lemmatizes words\n",
        "  # tokens = lemmatize_text_tokens(tokens)\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "mTNpdlBjsHjU"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_text_tokens(text_tokens):\n",
        "  # intializes lemmatizer\n",
        "  wnl = WordNetLemmatizer()\n",
        "\n",
        "  # pos tags\n",
        "  pos_tags = nltk.pos_tag(text_tokens)\n",
        "  # replaces NLTK tags with ones that are accepted by WordNetLemmatizer\n",
        "  pos_tags = [(word, lemmatizer_pos(tag)) for word, tag in pos_tags]\n",
        "\n",
        "  # lemmatizes each word\n",
        "  lemmatized_tokens = [wnl.lemmatize(word, tag) for word, tag in pos_tags]\n",
        "\n",
        "  return lemmatized_tokens"
      ],
      "metadata": {
        "id": "iFP7330H6krS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizer_pos(pos):\n",
        "\n",
        "  pos_first_letter = pos[0]\n",
        "\n",
        "  match pos_first_letter:\n",
        "    # verbs\n",
        "    case 'V':\n",
        "      pos = 'v'\n",
        "    # adjectives\n",
        "    case 'J':\n",
        "      pos = 'a'\n",
        "    # adverbs\n",
        "    case 'R':\n",
        "      pos = 'r'\n",
        "    case _:\n",
        "      pos = 'n'\n",
        "\n",
        "  return pos"
      ],
      "metadata": {
        "id": "vFQHx6nGAFau"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### adds a new column for the preprocessed review and populates it\n",
        "def preprocess_reviews_text(df):\n",
        "\n",
        "  if 'PREPROCESSED_REVIEW' not in df.columns:\n",
        "    df['PREPROCESSED_REVIEW'] = ''\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    review_text = row['REVIEW']\n",
        "\n",
        "    preprocessed_review = preprocess_text(review_text)\n",
        "    sub_df.at[index, 'PREPROCESSED_REVIEW'] = preprocessed_review\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "eQgP6iF7qy1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesses reviews text"
      ],
      "metadata": {
        "id": "Qadj5KlCJJT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# will preprocess text after getting a nice, relatively balanced version im happy with"
      ],
      "metadata": {
        "id": "hOtV1c21q46Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving files"
      ],
      "metadata": {
        "id": "gwgS9wcffeNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### downloading dataset\n",
        "\n",
        "# can change this if you wish\n",
        "filename = 'cleaned_goodreads_reviews_dataset.csv'\n",
        "\n",
        "reviews_df.to_csv(filename, index = False)\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "cSp2wGiefhB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}